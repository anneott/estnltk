{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function\n",
    "from estnltk import Layer\n",
    "from estnltk import EnvelopingBaseSpan\n",
    "from estnltk import EnvelopingSpan\n",
    "from estnltk import Annotation\n",
    "\n",
    "\"\"\"\n",
    "Module containing functionality for training and using NER models.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "tagger: NerTagger\n",
    "    Ner tagger with default model and parameters.\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import errno\n",
    "import inspect\n",
    "\n",
    "import six\n",
    "\n",
    "from estnltk.core import DEFAULT_PY2_NER_MODEL_DIR, DEFAULT_PY3_NER_MODEL_DIR\n",
    "from estnltk.taggers.estner.refac.names import *\n",
    "from estnltk.taggers.estner import Document, Sentence, Token\n",
    "from estnltk.taggers.estner import CrfsuiteTrainer, CrfsuiteTagger\n",
    "\n",
    "from estnltk.taggers.estner.featureextraction import FeatureExtractor\n",
    "\n",
    "# Use different NER models depending on Python version\n",
    "DEFAULT_NER_MODEL_DIR = DEFAULT_PY3_NER_MODEL_DIR if six.PY3 else DEFAULT_PY2_NER_MODEL_DIR\n",
    "\n",
    "\n",
    "class ModelStorageUtil(object):\n",
    "    def __init__(self, model_dir):\n",
    "        self.model_dir = model_dir\n",
    "        self.model_filename = os.path.join(model_dir, 'model.bin')\n",
    "        self.settings_filename = os.path.join(model_dir, 'settings.py')\n",
    "\n",
    "    def makedir(self):\n",
    "        \"\"\" Create model_dir directory \"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.model_dir)\n",
    "        except OSError as exception:\n",
    "            if exception.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    def copy_settings(self, settings_module):\n",
    "        \"\"\" Copy settings module to the model_dir directory \"\"\"\n",
    "        source = inspect.getsourcefile(settings_module)\n",
    "        dest = os.path.join(self.model_dir, 'settings.py')\n",
    "        shutil.copyfile(source, dest)\n",
    "\n",
    "    def load_settings(self):\n",
    "        \"\"\"Load settings module from the model_dir directory.\"\"\"\n",
    "        mname = 'loaded_module'\n",
    "        if six.PY2:\n",
    "            import imp\n",
    "            return imp.load_source(mname, self.settings_filename)\n",
    "        else:\n",
    "            import importlib.machinery\n",
    "            loader = importlib.machinery.SourceFileLoader(mname, self.settings_filename)\n",
    "        return loader.load_module(mname)\n",
    "\n",
    "\n",
    "def json_document_to_estner_document(jsondoc):\n",
    "    \"\"\"Convert an estnltk document to an estner document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsondoc: dict\n",
    "        Estnltk JSON-style document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    estnltk.estner.ner.Document\n",
    "        A ner document.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    snt = Sentence()\n",
    "    jsondoc.tag_layer()\n",
    "    zipped = list(zip(\n",
    "        jsondoc['words'],\n",
    "        jsondoc['morph_analysis']['lemma'],\n",
    "        jsondoc['morph_analysis']['root_tokens'],\n",
    "        jsondoc['morph_analysis']['form'],\n",
    "        jsondoc['morph_analysis']['ending'],\n",
    "        jsondoc['morph_analysis']['partofspeech']))\n",
    "    json_toks = [{TEXT: text, LEMMA: lemma, ROOT_TOKENS: root_tokens, FORM: form, ENDING: ending, POSTAG: postag}\n",
    "                 for text, lemma, root_tokens, form, ending, postag in zipped]\n",
    "    # add labels, if they are present\n",
    "    # also some 1.6 transformations\n",
    "    for tok, word in zip(json_toks, jsondoc.words):\n",
    "        tok['ending'] = tok['ending'][0]\n",
    "        tok['partofspeech'] = tok['partofspeech'][0]\n",
    "        tok['root_tokens'] = tok['root_tokens'][0]\n",
    "        tok['lemma'] = tok['lemma'][0]\n",
    "        tok['form'] = tok['form'][0]\n",
    "        tok['text'] = tok['text'].annotations[0].text\n",
    "        if LABEL in word.annotations:\n",
    "            tok[LABEL] = word.annotations[LABEL]\n",
    "    for json_tok in json_toks:\n",
    "        token = json_token_to_estner_token(json_tok)\n",
    "        snt.append(token)\n",
    "    if snt:\n",
    "        for i in range(1, len(snt)):\n",
    "            snt[i - 1].next = snt[i]\n",
    "            snt[i].prew = snt[i - 1]\n",
    "        sentences.append(snt)\n",
    "\n",
    "    return Document(sentences=sentences)\n",
    "\n",
    "\n",
    "def json_token_to_estner_token(json_token):\n",
    "    \"\"\"Convert a JSON-style word token to an estner token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vabamorf_token: dict\n",
    "        Vabamorf token representing a single word.\n",
    "    label: str\n",
    "        The label string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    estnltk.estner.ner.Token\n",
    "    \"\"\"\n",
    "    token = Token()\n",
    "    word = json_token[TEXT]\n",
    "    lemma = word\n",
    "    morph = ''\n",
    "    label = 'O'\n",
    "    ending = json_token[ENDING]\n",
    "    root_toks = json_token[ROOT_TOKENS]\n",
    "    if isinstance(root_toks, list):\n",
    "        root_toks = root_toks\n",
    "    lemma = '_'.join(root_toks) + ('+' + ending if ending else '')\n",
    "    if not lemma:\n",
    "        lemma = word\n",
    "    morph = '_%s_' % json_token[POSTAG]\n",
    "    morph += ' ' + json_token[FORM]\n",
    "    if LABEL in json_token:\n",
    "        label = json_token[LABEL]\n",
    "    return Token(word, lemma, morph, label)\n",
    "\n",
    "\n",
    "class NerTrainer(object):\n",
    "    \"\"\"The class for training NER models. Uses crfsuite implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, nersettings):\n",
    "        \"\"\"Initialize a new NerTrainer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        nersettings: module\n",
    "            NER settings module.\n",
    "        \"\"\"\n",
    "        self.settings = nersettings\n",
    "        self.fex = FeatureExtractor(nersettings)\n",
    "        self.trainer = CrfsuiteTrainer(algorithm=nersettings.CRFSUITE_ALGORITHM,\n",
    "                                       c2=nersettings.CRFSUITE_C2)\n",
    "\n",
    "    def train(self, jsondocs, model_dir):\n",
    "        \"\"\" Train a NER model using given documents.\n",
    "        \n",
    "        Each word in the documents must have a \"label\" attribute, which\n",
    "        denote the named entities in the documents.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        jsondocs: list of JSON-style documents.\n",
    "            The documents used for training the CRF model.\n",
    "        model_dir: str\n",
    "            A directory where the model will be saved.\n",
    "        \"\"\"\n",
    "        modelUtil = ModelStorageUtil(model_dir)\n",
    "        modelUtil.makedir()\n",
    "        modelUtil.copy_settings(self.settings)\n",
    "\n",
    "        # Convert json documents to ner documents\n",
    "        nerdocs = [json_document_to_estner_document(jsondoc)\n",
    "                   for jsondoc in jsondocs]\n",
    "\n",
    "        self.fex.prepare(nerdocs)\n",
    "        self.fex.process(nerdocs)\n",
    "\n",
    "        self.trainer.train(nerdocs, modelUtil.model_filename)\n",
    "\n",
    "\n",
    "class NerTagger(object):\n",
    "    \"\"\"The class for tagging named entities.\"\"\"\n",
    "\n",
    "    def __init__(self, model_dir=DEFAULT_NER_MODEL_DIR):\n",
    "        \"\"\"Initialize a new NerTagger instance.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_dir: st\n",
    "            A directory containing a trained ner model and a settings file.\n",
    "        \"\"\"\n",
    "        modelUtil = ModelStorageUtil(model_dir)\n",
    "        nersettings = modelUtil.load_settings()\n",
    "\n",
    "        self.fex = FeatureExtractor(nersettings)\n",
    "        self.tagger = CrfsuiteTagger(settings=nersettings,\n",
    "                                     model_filename=modelUtil.model_filename)\n",
    "\n",
    "    def tag_documents(self, documents):\n",
    "        nerdocs = [json_document_to_estner_document(jsondoc) for jsondoc in documents]\n",
    "        self.fex.process(nerdocs)\n",
    "        # add the labels\n",
    "        for nerdoc, jsondoc in zip(nerdocs, documents):\n",
    "            snt_labels = self.tagger.tag(nerdoc)\n",
    "            nerlayer = Layer(name=\"ner\", attributes=(\"nertag\",\"name\"), text_object=jsondoc, enveloping=\"words\")\n",
    "            entity_spans = []\n",
    "            for span,label in zip(jsondoc.words,snt_labels[0]):\n",
    "                if label == \"O\":\n",
    "                    if entity_spans:\n",
    "                        ebs = EnvelopingBaseSpan(entity_spans)\n",
    "                        envspan = EnvelopingSpan(ebs,nerlayer)\n",
    "                        annotation = Annotation(span=envspan,nertag = entity_type, name=\"po\")\n",
    "                        envspan.add_annotation(annotation)\n",
    "                        nerlayer.add_span(envspan)\n",
    "                        entity_spans = []\n",
    "                    continue\n",
    "                entity_type = label[2:]\n",
    "                entity_spans.append(span.base_span)\n",
    "\n",
    "            jsondoc.add_layer(nerlayer)\n",
    "        return documents\n",
    "\n",
    "    def tag_document(self, document):\n",
    "        return self.tag_documents([document])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst = {'paragraphs': [{'end': 92, 'start': 0}],\n",
    " 'sentences': [{'end': 38, 'start': 0}, {'end': 92, 'start': 39}],\n",
    " 'text': 'Eesti Vabariik on riik Põhja-Euroopas. Eesti piirneb põhjas üle Soome lahe Soome Vabariigiga',\n",
    " 'words': [{'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg g',\n",
    "     'lemma': 'Eesti',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Eesti',\n",
    "     'root_tokens': ['Eesti']}],\n",
    "   'end': 5,\n",
    "   'start': 0,\n",
    "   'text': 'Eesti'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg n',\n",
    "     'lemma': 'vabariik',\n",
    "     'partofspeech': 'S',\n",
    "     'root': 'vaba_riik',\n",
    "     'root_tokens': ['vaba', 'riik']}],\n",
    "   'end': 14,\n",
    "   'start': 6,\n",
    "   'text': 'Vabariik'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'b',\n",
    "     'lemma': 'olema',\n",
    "     'partofspeech': 'V',\n",
    "     'root': 'ole',\n",
    "     'root_tokens': ['ole']},\n",
    "    {'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'vad',\n",
    "     'lemma': 'olema',\n",
    "     'partofspeech': 'V',\n",
    "     'root': 'ole',\n",
    "     'root_tokens': ['ole']}],\n",
    "   'end': 17,\n",
    "   'start': 15,\n",
    "   'text': 'on'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg n',\n",
    "     'lemma': 'riik',\n",
    "     'partofspeech': 'S',\n",
    "     'root': 'riik',\n",
    "     'root_tokens': ['riik']}],\n",
    "   'end': 22,\n",
    "   'start': 18,\n",
    "   'text': 'riik'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': 's',\n",
    "     'form': 'sg in',\n",
    "     'lemma': 'Põhja-Euroobas',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Põhja-Eu_roobas',\n",
    "     'root_tokens': ['Põhja', 'Eu', 'roobas']},\n",
    "    {'clitic': '',\n",
    "     'ending': 's',\n",
    "     'form': 'sg in',\n",
    "     'lemma': 'Põhja-Euroopa',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Põhja-Euroopa',\n",
    "     'root_tokens': ['Põhja', 'Euroopa']}],\n",
    "   'end': 37,\n",
    "   'start': 23,\n",
    "   'text': 'Põhja-Euroopas'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '',\n",
    "     'form': '',\n",
    "     'lemma': '.',\n",
    "     'partofspeech': 'Z',\n",
    "     'root': '.',\n",
    "     'root_tokens': ['.']}],\n",
    "   'end': 38,\n",
    "   'start': 37,\n",
    "   'text': '.'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg n',\n",
    "     'lemma': 'Eesti',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Eesti',\n",
    "     'root_tokens': ['Eesti']}],\n",
    "   'end': 44,\n",
    "   'start': 39,\n",
    "   'text': 'Eesti'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': 'b',\n",
    "     'form': 'b',\n",
    "     'lemma': 'piirnema',\n",
    "     'partofspeech': 'V',\n",
    "     'root': 'piirne',\n",
    "     'root_tokens': ['piirne']}],\n",
    "   'end': 52,\n",
    "   'start': 45,\n",
    "   'text': 'piirneb'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': 's',\n",
    "     'form': 'sg in',\n",
    "     'lemma': 'põhi',\n",
    "     'partofspeech': 'S',\n",
    "     'root': 'põhi',\n",
    "     'root_tokens': ['põhi']}],\n",
    "   'end': 59,\n",
    "   'start': 53,\n",
    "   'text': 'põhjas'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': '',\n",
    "     'lemma': 'üle',\n",
    "     'partofspeech': 'D',\n",
    "     'root': 'üle',\n",
    "     'root_tokens': ['üle']}],\n",
    "   'end': 63,\n",
    "   'start': 60,\n",
    "   'text': 'üle'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg g',\n",
    "     'lemma': 'Soome',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Soome',\n",
    "     'root_tokens': ['Soome']}],\n",
    "   'end': 69,\n",
    "   'start': 64,\n",
    "   'text': 'Soome'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg g',\n",
    "     'lemma': 'laht',\n",
    "     'partofspeech': 'S',\n",
    "     'root': 'laht',\n",
    "     'root_tokens': ['laht']}],\n",
    "   'end': 74,\n",
    "   'start': 70,\n",
    "   'text': 'lahe'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': '0',\n",
    "     'form': 'sg n',\n",
    "     'lemma': 'Soome',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Soome',\n",
    "     'root_tokens': ['Soome']}],\n",
    "   'end': 80,\n",
    "   'start': 75,\n",
    "   'text': 'Soome'},\n",
    "  {'analysis': [{'clitic': '',\n",
    "     'ending': 'ga',\n",
    "     'form': 'sg kom',\n",
    "     'lemma': 'Vabariik',\n",
    "     'partofspeech': 'H',\n",
    "     'root': 'Vaba_riik',\n",
    "     'root_tokens': ['Vaba', 'riik']}],\n",
    "   'end': 92,\n",
    "   'start': 81,\n",
    "   'text': 'Vabariigiga'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = NerTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'tag_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cba23f8d81a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtekst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-02583f3a5bd9>\u001b[0m in \u001b[0;36mtag_document\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-02583f3a5bd9>\u001b[0m in \u001b[0;36mtag_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mnerdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjson_document_to_estner_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsondoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjsondoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnerdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;31m# add the labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-02583f3a5bd9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mnerdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjson_document_to_estner_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjsondoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjsondoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnerdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;31m# add the labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-02583f3a5bd9>\u001b[0m in \u001b[0;36mjson_document_to_estner_document\u001b[1;34m(jsondoc)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0msnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mjsondoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     zipped = list(zip(\n\u001b[0;32m     84\u001b[0m         \u001b[0mjsondoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'tag_layer'"
     ]
    }
   ],
   "source": [
    "nt.tag_document(tekst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
